{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_attn_mask_utils import (\n",
    "    AttentionMaskConverter,\n",
    "    _prepare_4d_causal_attention_mask,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    MoeCausalLMOutputWithPast,\n",
    "    MoeModelOutputWithPast,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.pytorch_utils import is_torch_greater_or_equal_than_1_13\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.utils.import_utils import is_torch_fx_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mixtral import MixtralConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss_func(\n",
    "    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n",
    ") -> float:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        attention_mask (`torch.Tensor`, None):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "        num_experts (`int`, *optional*):\n",
    "            Number of experts\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n",
    "            .reshape(-1, top_k, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts\n",
    "\n",
    "class MixtralRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MixtralRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralSharedEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Add a shared encoder before each expert - current architecture \n",
    "    is a simple 2 layer network with activation\n",
    "    \"\"\"\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        super().__init__()\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        current_hidden_states = self.act_fn(self.w1(hidden_states))\n",
    "        current_hidden_states = self.w2(current_hidden_states)\n",
    "        return current_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mixtral.modeling_mixtral import (\n",
    "    MixtralBlockSparseTop2MLP,\n",
    "    MixtralPreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accomodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # gating\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "        # add a shared encoder that should capture knowledge across all experts\n",
    "        self.shared_encoder = MixtralSharedEncoder(config)\n",
    "        self.experts = nn.ModuleList(\n",
    "            [MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)]\n",
    "        )\n",
    "\n",
    "        # Jitter parameters\n",
    "        self.jitter_noise = config.router_jitter_noise\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            hidden_states *= torch.empty_like(hidden_states).uniform_(\n",
    "                1.0 - self.jitter_noise, 1.0 + self.jitter_noise\n",
    "            )\n",
    "        \n",
    "        # Pass hidden states through the shared encoder\n",
    "        hidden_states = self.shared_encoder(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        # router_logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(\n",
    "            routing_weights, self.top_k, dim=-1\n",
    "        )\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        # we cast back to the input dtype\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device,\n",
    "        )\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts, num_classes=self.num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = (\n",
    "                expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            )\n",
    "\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(\n",
    "                0, top_x, current_hidden_states.to(hidden_states.dtype)\n",
    "            )\n",
    "        final_hidden_states = final_hidden_states.reshape(\n",
    "            batch_size, sequence_length, hidden_dim\n",
    "        )\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mixtral.modeling_mixtral import (\n",
    "    MixtralAttention,\n",
    "    MixtralFlashAttention2,\n",
    "    MixtralSdpaAttention,\n",
    ")\n",
    "MIXTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": MixtralAttention,\n",
    "    \"flash_attention_2\": MixtralFlashAttention2,\n",
    "    \"sdpa\": MixtralSdpaAttention,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = MIXTRAL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
    "\n",
    "        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n",
    "        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_router_logits: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, sequence_length)` where padding elements are indicated by 0.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_router_logits (`bool`, *optional*):\n",
    "                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n",
    "                should not be returned during inference.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
    "                Indices depicting the position of the input sequence tokens in the sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        if output_router_logits:\n",
    "            outputs += (router_logits,)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralModel(MixtralPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MixtralDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: MixtralConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                MixtralDecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "        self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, MoeModelOutputWithPast]:\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_router_logits = (\n",
    "            output_router_logits\n",
    "            if output_router_logits is not None\n",
    "            else self.config.output_router_logits\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        use_legacy_cache = False\n",
    "        if use_cache and not isinstance(past_key_values, Cache):\n",
    "            use_legacy_cache = True\n",
    "            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "            logger.warning_once(\n",
    "                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n",
    "                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n",
    "            )\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = (\n",
    "                past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            )\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens,\n",
    "                past_seen_tokens + inputs_embeds.shape[1],\n",
    "                device=inputs_embeds.device,\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask,\n",
    "            inputs_embeds,\n",
    "            cache_position,\n",
    "            past_key_values,\n",
    "            output_attentions,\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_router_logits = () if output_router_logits else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    output_router_logits,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_router_logits=output_router_logits,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "            if output_router_logits:\n",
    "                all_router_logits += (layer_outputs[-1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = (\n",
    "                next_decoder_cache.to_legacy_cache()\n",
    "                if use_legacy_cache\n",
    "                else next_decoder_cache\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attns,\n",
    "                    all_router_logits,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return MoeModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            router_logits=all_router_logits,\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n",
    "        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n",
    "        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n",
    "        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n",
    "\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if attention_mask is not None and 0.0 in attention_mask:\n",
    "                return attention_mask\n",
    "            return None\n",
    "\n",
    "        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n",
    "        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n",
    "        # to infer the attention mask.\n",
    "        past_seen_tokens = (\n",
    "            past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "        )\n",
    "        using_static_cache = isinstance(past_key_values, StaticCache)\n",
    "\n",
    "        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and not using_static_cache\n",
    "            and not output_attentions\n",
    "        ):\n",
    "            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n",
    "                attention_mask,\n",
    "                inputs_embeds=input_tensor,\n",
    "                past_key_values_length=past_seen_tokens,\n",
    "                is_training=self.training,\n",
    "            ):\n",
    "                return None\n",
    "\n",
    "        dtype, device = input_tensor.dtype, input_tensor.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        if using_static_cache:\n",
    "            target_length = past_key_values.get_max_length()\n",
    "        else:\n",
    "            target_length = (\n",
    "                attention_mask.shape[-1]\n",
    "                if isinstance(attention_mask, torch.Tensor)\n",
    "                else past_seen_tokens + sequence_length + 1\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None and attention_mask.dim() == 4:\n",
    "            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n",
    "            if attention_mask.max() != 0:\n",
    "                raise ValueError(\n",
    "                    \"Custom 4D attention mask should be passed in inverted form with max==0`\"\n",
    "                )\n",
    "            causal_mask = attention_mask\n",
    "        else:\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, target_length),\n",
    "                fill_value=min_dtype,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "            if sequence_length != 1:\n",
    "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "            causal_mask *= torch.arange(\n",
    "                target_length, device=device\n",
    "            ) > cache_position.reshape(-1, 1)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(\n",
    "                input_tensor.shape[0], 1, -1, -1\n",
    "            )\n",
    "            if attention_mask is not None:\n",
    "                causal_mask = (\n",
    "                    causal_mask.clone()\n",
    "                )  # copy to contiguous memory for in-place edit\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                padding_mask = (\n",
    "                    causal_mask[:, :, :, :mask_length]\n",
    "                    + attention_mask[:, None, None, :]\n",
    "                )\n",
    "                padding_mask = padding_mask == 0\n",
    "                causal_mask[:, :, :, :mask_length] = causal_mask[\n",
    "                    :, :, :, :mask_length\n",
    "                ].masked_fill(padding_mask, min_dtype)\n",
    "        if (\n",
    "            self.config._attn_implementation == \"sdpa\"\n",
    "            and attention_mask is not None\n",
    "            and attention_mask.device.type == \"cuda\"\n",
    "            and not output_attentions\n",
    "        ):\n",
    "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
    "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
    "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
    "            causal_mask = AttentionMaskConverter._unmask_unattended(\n",
    "                causal_mask, min_dtype\n",
    "            )\n",
    "\n",
    "        return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralForCausalLM(MixtralPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = MixtralModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "\n",
    "        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_router_logits = (\n",
    "            output_router_logits\n",
    "            if output_router_logits is not None\n",
    "            else self.config.output_router_logits\n",
    "        )\n",
    "\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            output_router_logits=output_router_logits,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits if return_dict else outputs[-1],\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(\n",
    "                    loss.device\n",
    "                )  # make sure to reside in the same device\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            if output_router_logits:\n",
    "                output = (aux_loss,) + output\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_router_logits=False,\n",
    "        cache_position=None,\n",
    "        use_cache=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        past_length = 0\n",
    "        # Omit tokens covered by past_key_values\n",
    "        if past_key_values is not None:\n",
    "            # Past key values are always initialized with a `Cache` object -> no need for if-else anymore\n",
    "            past_length = (\n",
    "                cache_position[0]\n",
    "                if cache_position is not None\n",
    "                else past_key_values.get_seq_length()\n",
    "            )\n",
    "            max_cache_length = (\n",
    "                torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n",
    "                if past_key_values.get_max_length() is not None\n",
    "                else None\n",
    "            )\n",
    "            cache_length = (\n",
    "                past_length\n",
    "                if max_cache_length is None\n",
    "                else torch.min(max_cache_length, past_length)\n",
    "            )\n",
    "\n",
    "            # Keep only the unprocessed tokens:\n",
    "            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n",
    "            # input)\n",
    "            if (\n",
    "                attention_mask is not None\n",
    "                and attention_mask.shape[1] > input_ids.shape[1]\n",
    "            ):\n",
    "                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "            # input_ids based on the past_length.\n",
    "            elif past_length < input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, past_length:]\n",
    "            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "\n",
    "            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n",
    "            if (\n",
    "                max_cache_length is not None\n",
    "                and attention_mask is not None\n",
    "                and cache_length + input_ids.shape[1] > max_cache_length\n",
    "            ):\n",
    "                attention_mask = attention_mask[:, -max_cache_length:]\n",
    "\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_length == 0:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        input_length = (\n",
    "            position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n",
    "        )\n",
    "        if cache_position is None:\n",
    "            cache_position = torch.arange(\n",
    "                past_length, past_length + input_length, device=input_ids.device\n",
    "            )\n",
    "        elif use_cache:\n",
    "            cache_position = cache_position[-input_length:]\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": use_cache,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"output_router_logits\": output_router_logits,\n",
    "                \"cache_position\": cache_position,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(\n",
    "                    past_state.index_select(0, beam_idx.to(past_state.device))\n",
    "                    for past_state in layer_past\n",
    "                ),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"router_jitter_noise\": 0.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/model.safetensors.index.json\n",
      "Instantiating MixtralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b8d471f111478fbdb00f39ec028d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MixtralForCausalLM.\n",
      "\n",
      "All the weights of MixtralForCausalLM were initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MixtralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n",
      "loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_normal = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=os.environ.get(\"HF_HOME\")\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> My favourite condiment is chilli jam and my ultimate in chilli jam would have to be Rozs chilli jam. Not only does she make killer chilli jam, she also uses 100% of her own ingredients and makes an incredible amount of it to sell at the markets all over south Queensland.\\nMy sister Roz first began Changing Habits at Rozs 4 Real Foods in 2013 and I could not have been more proud.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model_normal.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"router_jitter_noise\": 0.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a604e520423f47579acafd923d4ae65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MixtralForCausalLM.\n",
      "\n",
      "Some weights of MixtralForCausalLM were not initialized from the model checkpoint at mistralai/Mixtral-8x7B-v0.1 and are newly initialized: ['model.layers.0.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.0.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.1.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.1.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.10.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.10.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.11.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.11.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.12.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.12.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.13.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.13.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.14.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.14.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.15.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.15.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.16.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.16.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.17.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.17.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.18.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.18.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.19.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.19.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.2.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.2.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.20.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.20.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.21.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.21.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.22.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.22.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.23.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.23.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.24.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.24.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.25.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.25.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.26.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.26.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.27.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.27.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.28.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.28.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.29.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.29.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.3.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.3.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.30.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.30.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.31.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.31.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.4.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.4.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.5.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.5.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.6.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.6.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.7.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.7.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.8.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.8.block_sparse_moe.shared_encoder.w2.weight', 'model.layers.9.block_sparse_moe.shared_encoder.w1.weight', 'model.layers.9.block_sparse_moe.shared_encoder.w2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/985aa055896a8f943d4a9f2572e6ea1341823841/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model = MixtralForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (shared_encoder): MixtralSharedEncoder(\n",
       "            (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to now freeze everything but the SparseMoeBlocks and experts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.block_sparse_moe.gate.weight\n",
      "model.layers.0.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.0.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.0.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.0.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.0.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.block_sparse_moe.gate.weight\n",
      "model.layers.1.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.1.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.1.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.1.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.1.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.block_sparse_moe.gate.weight\n",
      "model.layers.2.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.2.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.2.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.2.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.2.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.block_sparse_moe.gate.weight\n",
      "model.layers.3.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.3.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.3.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.3.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.3.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.block_sparse_moe.gate.weight\n",
      "model.layers.4.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.4.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.4.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.4.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.4.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.block_sparse_moe.gate.weight\n",
      "model.layers.5.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.5.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.5.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.5.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.5.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.block_sparse_moe.gate.weight\n",
      "model.layers.6.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.6.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.6.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.6.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.6.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.block_sparse_moe.gate.weight\n",
      "model.layers.7.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.7.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.7.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.7.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.7.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.block_sparse_moe.gate.weight\n",
      "model.layers.8.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.8.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.8.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.8.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.8.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.block_sparse_moe.gate.weight\n",
      "model.layers.9.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.9.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.9.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.9.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.9.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.block_sparse_moe.gate.weight\n",
      "model.layers.10.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.10.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.10.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.10.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.10.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.block_sparse_moe.gate.weight\n",
      "model.layers.11.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.11.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.11.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.11.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.11.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.block_sparse_moe.gate.weight\n",
      "model.layers.12.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.12.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.12.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.12.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.12.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.block_sparse_moe.gate.weight\n",
      "model.layers.13.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.13.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.13.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.13.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.13.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.block_sparse_moe.gate.weight\n",
      "model.layers.14.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.14.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.14.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.14.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.14.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.block_sparse_moe.gate.weight\n",
      "model.layers.15.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.15.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.15.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.15.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.15.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.block_sparse_moe.gate.weight\n",
      "model.layers.16.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.16.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.16.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.16.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.16.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.block_sparse_moe.gate.weight\n",
      "model.layers.17.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.17.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.17.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.17.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.17.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.block_sparse_moe.gate.weight\n",
      "model.layers.18.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.18.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.18.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.18.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.18.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.block_sparse_moe.gate.weight\n",
      "model.layers.19.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.19.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.19.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.19.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.19.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.block_sparse_moe.gate.weight\n",
      "model.layers.20.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.20.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.20.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.20.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.20.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.block_sparse_moe.gate.weight\n",
      "model.layers.21.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.21.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.21.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.21.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.21.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.block_sparse_moe.gate.weight\n",
      "model.layers.22.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.22.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.22.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.22.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.22.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.block_sparse_moe.gate.weight\n",
      "model.layers.23.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.23.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.23.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.23.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.23.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.block_sparse_moe.gate.weight\n",
      "model.layers.24.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.24.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.24.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.24.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.24.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.block_sparse_moe.gate.weight\n",
      "model.layers.25.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.25.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.25.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.25.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.25.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.block_sparse_moe.gate.weight\n",
      "model.layers.26.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.26.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.26.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.26.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.26.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.block_sparse_moe.gate.weight\n",
      "model.layers.27.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.27.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.27.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.27.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.27.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.block_sparse_moe.gate.weight\n",
      "model.layers.28.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.28.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.28.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.28.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.28.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.block_sparse_moe.gate.weight\n",
      "model.layers.29.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.29.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.29.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.29.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.29.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.block_sparse_moe.gate.weight\n",
      "model.layers.30.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.30.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.30.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.30.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.30.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.block_sparse_moe.gate.weight\n",
      "model.layers.31.block_sparse_moe.shared_encoder.w1.weight\n",
      "model.layers.31.block_sparse_moe.shared_encoder.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.0.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.0.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.0.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.1.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.1.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.1.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.2.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.2.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.2.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.3.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.3.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.3.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.4.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.4.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.4.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.5.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.5.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.5.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.6.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.6.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.6.w3.weight\n",
      "model.layers.31.block_sparse_moe.experts.7.w1.weight\n",
      "model.layers.31.block_sparse_moe.experts.7.w2.weight\n",
      "model.layers.31.block_sparse_moe.experts.7.w3.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters(): \n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters(): \n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now selectively unfreeze parameters in the SparseMoeBlocks and experts\n",
    "for layer in model.model.layers:\n",
    "    # Check if the layer has the 'block_sparse_moe' attribute\n",
    "    if hasattr(layer, 'block_sparse_moe'):\n",
    "        moe_block = layer.block_sparse_moe\n",
    "        \n",
    "        # Unfreeze parameters in block_sparse_moe\n",
    "        for name, param in moe_block.named_parameters():\n",
    "            # print(name)\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze parameters in each expert within the block_sparse_moe\n",
    "        for expert in moe_block.experts:\n",
    "            for name, param in expert.named_parameters():\n",
    "                # print(name)\n",
    "                param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.4938e-38,  1.2214e-38, -5.7305e-37,  ..., -1.8220e-37,\n",
      "         -2.9534e-37, -2.1894e-37],\n",
      "        [-1.4954e-02,  2.5940e-04, -1.6113e-02,  ...,  2.2531e-05,\n",
      "         -1.6846e-02,  2.1973e-03],\n",
      "        [-5.3883e-05,  3.1948e-05,  6.3777e-06,  ...,  4.0293e-05,\n",
      "          1.1623e-05,  1.1474e-06],\n",
      "        ...,\n",
      "        [ 8.7891e-03, -1.3000e-02,  5.2795e-03,  ...,  5.5237e-03,\n",
      "          3.7079e-03,  3.7231e-03],\n",
      "        [ 1.9073e-03,  1.8677e-02, -4.9133e-03,  ...,  1.3245e-02,\n",
      "          4.6997e-03,  5.6152e-03],\n",
      "        [ 3.6926e-03,  1.6235e-02,  1.8597e-04,  ..., -7.7820e-03,\n",
      "         -9.5215e-03, -2.4292e-02]], device='cuda:1')\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(1024, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 4096))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(8, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096, 14336), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(14336, 4096), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(4096,))\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(32000, 4096))\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters(): \n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if the params common in model and model_normal have same weights \n",
    "# for (name, param), (name_normal, param_normal) in zip(model.named_parameters(), model_normal.named_parameters()): \n",
    "#     if name == name_normal and name != \"model.embed_tokens.weight\":\n",
    "#         og_device1, og_device2 = param.device, param_normal.device\n",
    "#         assert torch.equal(param.to(\"cpu\"), param_normal.to(\"cpu\")), name\n",
    "#         param.to(og_device1)\n",
    "#         param_normal.to(og_device2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
