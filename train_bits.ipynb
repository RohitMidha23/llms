{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/vol/bitbucket/rm1623/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/vol/bitbucket/rm1623/.cache/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/rm1623/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import wandb\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/opt-350m'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50272, 16, 65536)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "vocab_size = model.config.vocab_size\n",
    "bit_length = math.ceil(math.log2(vocab_size))\n",
    "vocab_size, bit_length, 2**bit_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): BitStringOutputLayer(\n",
       "    (linear): Linear(in_features=512, out_features=16, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BitStringOutputLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, bit_length):\n",
    "        super(BitStringOutputLayer, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_dim, bit_length)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "# model.lm_head = BitStringOutputLayer(model.config.hidden_size, bit_length)\n",
    "model.lm_head = BitStringOutputLayer(model.config.word_embed_proj_dim, bit_length)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithPast():\n",
    "    \"\"\"\n",
    "    Base class for causal language model (or autoregressive) outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
    "            Language modeling loss (for next-token prediction).\n",
    "        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
    "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
    "            `past_key_values` input) to speed up sequential decoding.\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
    "            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
    "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
    "            sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTPreTrainedModel, OPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"_name_or_path\": \"opt-350m\",\n",
    "  \"activation_dropout\": 0.0,\n",
    "  \"activation_function\": \"relu\",\n",
    "  \"architectures\": [\n",
    "    \"OPTForCausalLM\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 2,\n",
    "  \"do_layer_norm_before\": False,\n",
    "  \"dropout\": 0.1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"ffn_dim\": 4096,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"init_std\": 0.02,\n",
    "  \"layerdrop\": 0.0,\n",
    "  \"max_position_embeddings\": 2048,\n",
    "  \"model_type\": \"opt\",\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"prefix\": \"</s>\",\n",
    "  \"torch_dtype\": \"float16\",\n",
    "  \"transformers_version\": \"4.20.0.dev0\",\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50272,\n",
    "  \"word_embed_proj_dim\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmodel = OPTModel.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = optmodel(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 31414,     6,   127,  2335,    16, 11962]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 512])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head = nn.Linear(512, 16, bias=False)\n",
    "\n",
    "logits = lm_head(sample_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 16]), torch.Size([1, 6]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "shift_logits.shape, shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2454,  5.2966, -1.3153, -0.4532, -0.8342,  0.5373, -0.4919,  1.9976,\n",
       "        -3.5326,  2.9077,  1.4376, -1.5307, -0.4819,  2.5617, -3.1072, -0.5668],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31414,     6,   127,  2335,    16, 11962]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OPTForCausalLM(OPTPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = OPTModel(config)\n",
    "\n",
    "        # the lm_head weight is automatically tied to the embed tokens weight\n",
    "        self.bit_size = torch.log2(torch.tensor(config.vocab_size)).ceil().int().item()\n",
    "        self.lm_head = nn.Linear(config.word_embed_proj_dim, self.bit_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.decoder.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model.decoder = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.decoder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(outputs[0]).contiguous()\n",
    "        \n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def hamming_loss(predicted, target):\n",
    "    \"\"\"Compute the Hamming loss between predicted and target bit strings\"\"\"\n",
    "    return (predicted != target).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = dataset['train'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 5/5 [00:00<00:00, 307.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "texts = dataset[\"train\"][\"text\"]\n",
    "block_size = 10\n",
    "sample_tokenized_datasets = sample_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=['instruction', 'input', 'output', 'text'],\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "\n",
    "def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        print(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 10: 100%|██████████| 5/5 [00:00<00:00, 306.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 45943, 16, 41, 15741, 14, 7448, 10, 3685, 4], [21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4, 50118], [50118, 48134, 41241, 35, 50118, 31033, 130, 4965, 13, 4959], [2245, 4, 50118, 50118, 48134, 19121, 35, 50118, 134, 4], [43800, 10, 9320, 5626, 8, 146, 686, 7, 680, 2710], [9, 12849, 8, 8942, 4, 1437, 50118, 176, 4, 30450], [4595, 7, 489, 110, 809, 2171, 8, 670, 4, 1437], [50118, 246, 4, 2315, 615, 3581, 8, 3014, 10, 4292], [3581, 3078, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 2264, 32], [5, 130, 2270, 8089, 116, 50118, 50118, 48134, 19121, 35], [50118, 133, 130, 2270, 8089, 32, 1275, 6, 2440, 6], [8, 5718, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 47066, 21700], [5, 3184, 9, 41, 37113, 4, 50118, 50118, 48134, 19121], [35, 50118, 4688, 37113, 16, 156, 62, 9, 10, 38531], [6, 61, 6308, 24892, 1790, 8, 28979, 26573, 6, 7501], [30, 36522, 14, 1504, 11, 38786, 198, 5, 38531, 4], [20, 24892, 1790, 8, 28979, 26573, 33, 10, 1313, 1427], [6, 150, 5, 36522, 33, 10, 2430, 1427, 6, 5203], [11, 41, 1374, 7974, 37113, 4, 20, 346, 9, 349], [33100, 23483, 5, 21495, 346, 8, 5, 1907, 9, 37113], [4, 2, 45943, 16, 41, 15741, 14, 7448, 10, 3685], [4, 21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4], [50118, 50118, 48134, 41241, 35, 50118, 6179, 64, 52, 1888], [935, 6631, 116, 50118, 50118, 48134, 19121, 35, 50118, 970], [32, 10, 346, 9, 1319, 7, 1888, 935, 6631, 6], [215, 25, 9255, 7, 8741, 1007, 1715, 6, 5513, 5], [304, 9, 285, 4264, 6, 27043, 5, 6574, 9, 11422], [12174, 6, 9704, 1986, 7, 1888, 5035, 31, 2683, 1715], [6, 8, 9704, 1155, 5035, 2820, 4, 6903, 6, 2172], [64, 109, 49, 233, 7, 1888, 935, 6631, 30, 4881], [512, 304, 6, 11473, 6574, 3183, 215, 25, 5627, 6], [8, 2992, 7, 1007, 5693, 14636, 4, 2, 45943, 16], [41, 15741, 14, 7448, 10, 3685, 4, 21062, 10, 1263], [14, 16574, 25830, 5, 2069, 4, 50118, 50118, 48134, 41241], [35, 50118, 47066, 21700, 10, 86, 77, 47, 56, 7], [146, 10, 1202, 568, 4, 50118, 50118, 48134, 19121, 35], [50118, 100, 56, 7, 146, 10, 1202, 568, 77, 38], [21, 447, 25, 10, 695, 1044, 23, 10, 1663, 138], [4, 38, 21, 11, 1427, 9, 10, 695, 14, 956], [7, 28, 2121, 30, 10, 1402, 1248, 11, 645, 7], [972, 5, 3653, 17, 27, 29, 2113, 4, 635, 6], [528, 7, 7152, 6091, 6, 52, 58, 45, 441, 7], [972, 5, 4267, 8, 98, 38, 56, 7, 146, 10], [1202, 568, 4, 38, 1276, 7, 4442, 5, 4267, 6], [53, 38, 56, 7, 4140, 5, 165, 17, 27, 29], [1915, 190, 617, 8, 712, 5, 1229, 4, 2223, 24], [21, 10, 11788, 568, 6, 38, 3284, 1276, 7, 213], [789, 19, 24, 7, 1306, 14, 5, 695, 21, 2121], [15, 86, 8, 14, 5, 3653, 17, 27, 29, 2113], [58, 1145, 4, 20, 695, 21, 2140, 5116, 2121, 8], [42, 21, 450, 25, 10, 17750, 7, 127, 1673, 8]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2, 45943, 16, 41, 15741, 14, 7448, 10, 3685, 4], [21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4, 50118], [50118, 48134, 41241, 35, 50118, 31033, 130, 4965, 13, 4959], [2245, 4, 50118, 50118, 48134, 19121, 35, 50118, 134, 4], [43800, 10, 9320, 5626, 8, 146, 686, 7, 680, 2710], [9, 12849, 8, 8942, 4, 1437, 50118, 176, 4, 30450], [4595, 7, 489, 110, 809, 2171, 8, 670, 4, 1437], [50118, 246, 4, 2315, 615, 3581, 8, 3014, 10, 4292], [3581, 3078, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 2264, 32], [5, 130, 2270, 8089, 116, 50118, 50118, 48134, 19121, 35], [50118, 133, 130, 2270, 8089, 32, 1275, 6, 2440, 6], [8, 5718, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 47066, 21700], [5, 3184, 9, 41, 37113, 4, 50118, 50118, 48134, 19121], [35, 50118, 4688, 37113, 16, 156, 62, 9, 10, 38531], [6, 61, 6308, 24892, 1790, 8, 28979, 26573, 6, 7501], [30, 36522, 14, 1504, 11, 38786, 198, 5, 38531, 4], [20, 24892, 1790, 8, 28979, 26573, 33, 10, 1313, 1427], [6, 150, 5, 36522, 33, 10, 2430, 1427, 6, 5203], [11, 41, 1374, 7974, 37113, 4, 20, 346, 9, 349], [33100, 23483, 5, 21495, 346, 8, 5, 1907, 9, 37113], [4, 2, 45943, 16, 41, 15741, 14, 7448, 10, 3685], [4, 21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4], [50118, 50118, 48134, 41241, 35, 50118, 6179, 64, 52, 1888], [935, 6631, 116, 50118, 50118, 48134, 19121, 35, 50118, 970], [32, 10, 346, 9, 1319, 7, 1888, 935, 6631, 6], [215, 25, 9255, 7, 8741, 1007, 1715, 6, 5513, 5], [304, 9, 285, 4264, 6, 27043, 5, 6574, 9, 11422], [12174, 6, 9704, 1986, 7, 1888, 5035, 31, 2683, 1715], [6, 8, 9704, 1155, 5035, 2820, 4, 6903, 6, 2172], [64, 109, 49, 233, 7, 1888, 935, 6631, 30, 4881], [512, 304, 6, 11473, 6574, 3183, 215, 25, 5627, 6], [8, 2992, 7, 1007, 5693, 14636, 4, 2, 45943, 16], [41, 15741, 14, 7448, 10, 3685, 4, 21062, 10, 1263], [14, 16574, 25830, 5, 2069, 4, 50118, 50118, 48134, 41241], [35, 50118, 47066, 21700, 10, 86, 77, 47, 56, 7], [146, 10, 1202, 568, 4, 50118, 50118, 48134, 19121, 35], [50118, 100, 56, 7, 146, 10, 1202, 568, 77, 38], [21, 447, 25, 10, 695, 1044, 23, 10, 1663, 138], [4, 38, 21, 11, 1427, 9, 10, 695, 14, 956], [7, 28, 2121, 30, 10, 1402, 1248, 11, 645, 7], [972, 5, 3653, 17, 27, 29, 2113, 4, 635, 6], [528, 7, 7152, 6091, 6, 52, 58, 45, 441, 7], [972, 5, 4267, 8, 98, 38, 56, 7, 146, 10], [1202, 568, 4, 38, 1276, 7, 4442, 5, 4267, 6], [53, 38, 56, 7, 4140, 5, 165, 17, 27, 29], [1915, 190, 617, 8, 712, 5, 1229, 4, 2223, 24], [21, 10, 11788, 568, 6, 38, 3284, 1276, 7, 213], [789, 19, 24, 7, 1306, 14, 5, 695, 21, 2121], [15, 86, 8, 14, 5, 3653, 17, 27, 29, 2113], [58, 1145, 4, 20, 695, 21, 2140, 5116, 2121, 8], [42, 21, 450, 25, 10, 17750, 7, 127, 1673, 8]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_lm_datasets = sample_tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            load_from_cache_file=False,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 55\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        sample_lm_datasets, shuffle=True, collate_fn=default_data_collator, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "gradient_accumulation_steps = 2\n",
    "num_warmup_steps = 5\n",
    "num_processes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "overrode_max_train_steps = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps * num_processes,\n",
    "        num_training_steps=max_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, None, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = 100\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "total_batch_size = per_device_train_batch_size * num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [03:53,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "with_tracking = True\n",
    "\n",
    "resume_from_checkpoint = False\n",
    "starting_epoch=0\n",
    "completed_steps = 0\n",
    "\n",
    "resume_step = None\n",
    "\n",
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "per_device_eval_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 50272]' is invalid for input of size 144",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(active_dataloader):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m---> 12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# We keep track of the loss at each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/rm1623/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:1168\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1168\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m, shift_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1171\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 50272]' is invalid for input of size 144"
     ]
    }
   ],
   "source": [
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "        model.train()\n",
    "        if with_tracking:\n",
    "            total_loss = 0\n",
    "        if resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "        else:\n",
    "            active_dataloader = train_dataloader\n",
    "        for step, batch in enumerate(active_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # We keep track of the loss at each epoch\n",
    "                if with_tracking:\n",
    "                    total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    accelerator.save_state(output_dir)\n",
    "            if completed_steps >= max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        if eval_dataloader is not None:\n",
    "            for step, batch in enumerate(eval_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**batch)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                losses.append(accelerator.gather_for_metrics(loss.repeat(per_device_eval_batch_size)))\n",
    "\n",
    "            losses = torch.cat(losses)\n",
    "            try:\n",
    "                eval_loss = torch.mean(losses)\n",
    "                perplexity = math.exp(eval_loss)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "\n",
    "            print(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "            if with_tracking:\n",
    "                accelerator.log(\n",
    "                    {\n",
    "                        \"perplexity\": perplexity,\n",
    "                        \"eval_loss\": eval_loss,\n",
    "                        \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": completed_steps,\n",
    "                    },\n",
    "                    step=completed_steps,\n",
    "                )\n",
    "\n",
    "\n",
    "if with_tracking:\n",
    "    accelerator.end_training()\n",
    "\n",
    "# if args.output_dir is not None:\n",
    "#     accelerator.wait_for_everyone()\n",
    "#     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     unwrapped_model.save_pretrained(\n",
    "#         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "#     )\n",
    "#     if accelerator.is_main_process:\n",
    "#         tokenizer.save_pretrained(args.output_dir)\n",
    "#         if args.push_to_hub:\n",
    "#             api.upload_folder(\n",
    "#                 commit_message=\"End of training\",\n",
    "#                 folder_path=args.output_dir,\n",
    "#                 repo_id=repo_id,\n",
    "#                 repo_type=\"model\",\n",
    "#                 token=args.hub_token,\n",
    "#             )\n",
    "#         with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "#             json.dump({\"perplexity\": perplexity}, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_outputs = model(input_ids[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1511, 16])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_outputs.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6652, 0.3778, 0.0269,  ..., 0.0633, 0.9739, 0.5414],\n",
       "        [0.3898, 0.5647, 0.1780,  ..., 0.8867, 0.0281, 0.7063],\n",
       "        [0.4054, 0.6966, 0.8962,  ..., 0.9953, 0.0642, 0.9606],\n",
       "        ...,\n",
       "        [0.7411, 0.1787, 0.0168,  ..., 0.0109, 0.9965, 0.8426],\n",
       "        [0.7360, 0.1742, 0.0163,  ..., 0.0109, 0.9966, 0.8393],\n",
       "        [0.7358, 0.1741, 0.0164,  ..., 0.0107, 0.9966, 0.8414]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1511])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(x):\n",
    "    return x > 0.5  # 0.5 is the threshold\n",
    "\n",
    "# to_binary(example_outputs.logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_mlm(texts):\n",
    "    inputs = tokenizer(texts, padding=True, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_masks = inputs[\"attention_mask\"]\n",
    "    \n",
    "    rng = np.random.default_rng()  # Random number generator\n",
    "    mask_arr = rng.random(input_ids.shape) < 0.15  # Mask 15% of all tokens\n",
    "    mask_arr = mask_arr & (input_ids != tokenizer.pad_token_id)  # Do not mask padding tokens\n",
    "\n",
    "    # Replace 80% of masked positions with [MASK] token, 10% with random token, 10% unchanged\n",
    "    mask_positions = np.where(mask_arr)\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    for i in range(mask_positions[0].size):\n",
    "        rand = rng.random()\n",
    "        if rand < 0.8:  # 80% replaced by [MASK]\n",
    "            masked_input_ids[mask_positions[0][i], mask_positions[1][i]] = tokenizer.mask_token_id\n",
    "        elif rand < 0.9:  # 10% replaced by random token\n",
    "            masked_input_ids[mask_positions[0][i], mask_positions[1][i]] = rng.integers(1, tokenizer.vocab_size)\n",
    "    labels = -100 * torch.ones(input_ids.shape, dtype=torch.long)\n",
    "    labels[mask_arr] = input_ids[mask_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the existing parameters for the base model \n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids)\n",
    "    predictions = model.lm_head(outputs.last_hidden_state)\n",
    "    loss = loss_fn(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size, bit_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0, vocab_size, (1, 10))  # Example input\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
