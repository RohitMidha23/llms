{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/vol/bitbucket/rm1623/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"HF_HOME\"] = \"/vol/bitbucket/rm1623/.cache/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/rm1623/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import wandb\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/opt-350m'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from dataclasses import fields, is_dataclass\n",
    "from functools import partial\n",
    "from typing import Any, List, Tuple, Iterable\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTPreTrainedModel, OPTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OPT to understand logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmodel = OPTModel.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = optmodel(**tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 31414,     6,   127,  2335,    16, 11962]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 512])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head = nn.Linear(512, 16, bias=False)\n",
    "\n",
    "logits = lm_head(sample_output[0])\n",
    "\n",
    "logits = nn.Sigmoid()(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 16]), torch.Size([1, 6]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "shift_logits.shape, shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8788, 0.8846, 0.9053, 0.6172, 0.9827, 0.8940, 0.5551, 0.7627, 0.6673,\n",
       "        0.9324, 0.2848, 0.9559, 0.3093, 0.0927, 0.5010, 0.9839],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def int_to_bin_tensor(val, length=16):\n",
    "    bin_str = format(val, '0' + str(length) + 'b')\n",
    "    bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "    return bin_tensor\n",
    "# int_to_bin_tensor(2)\n",
    "\n",
    "binary_tensors = [int_to_bin_tensor(val.item()) for val in shift_labels.flatten()]\n",
    "\n",
    "\n",
    "binary_tensors = torch.stack(binary_tensors).view(*shift_labels.shape, -1)\n",
    "\n",
    "binary_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.L1Loss()\n",
    "loss = loss_fct(shift_logits, binary_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4442, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify OPT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OPTForCausalLM(OPTPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = OPTModel(config)\n",
    "\n",
    "        # the lm_head weight is automatically tied to the embed tokens weight\n",
    "        self.bit_size = torch.log2(torch.tensor(config.vocab_size)).ceil().int().item()\n",
    "        self.lm_head = nn.Sequential(\n",
    "            nn.Linear(config.word_embed_proj_dim, self.bit_size, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.decoder.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model.decoder = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.decoder\n",
    "    \n",
    "    def int_to_bin_tensor(self, val):\n",
    "        length = self.bit_size\n",
    "        bin_str = format(val, '0' + str(length) + 'b')\n",
    "        bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "        return bin_tensor\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(outputs[0]).contiguous()  # (bs, seq_length, bit_size)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # convert the labels to binary - currently they are indexes of the tokenizer\n",
    "            binary_tensors = [self.int_to_bin_tensor(val.item()) for val in shift_labels.flatten()]\n",
    "            # get the binary tokens in the same shape as the original tensor\n",
    "            binary_tensors = torch.stack(binary_tensors).view(*shift_labels.shape, -1)\n",
    "            binary_tensors = binary_tensors.to(logits.device)\n",
    "            # add L1 loss\n",
    "            loss_fct = nn.L1Loss()\n",
    "            loss = loss_fct(shift_logits, binary_tensors)\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /homes/rm1623/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/config.json\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"enable_bias\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_elementwise_affine\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /homes/rm1623/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing OPTForCausalLM.\n",
      "\n",
      "Some weights of OPTForCausalLM were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['lm_head.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file generation_config.json from cache at /homes/rm1623/.cache/huggingface/hub/models--facebook--opt-350m/snapshots/08ab08cc4b72ff5593870b5d527cf4230323703c/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=16, bias=False)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def hamming_loss(predicted, target):\n",
    "#     \"\"\"Compute the Hamming loss between predicted and target bit strings\"\"\"\n",
    "#     return (predicted != target).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = dataset['train'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 5/5 [00:00<00:00, 305.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "texts = dataset[\"train\"][\"text\"]\n",
    "block_size = 10\n",
    "sample_tokenized_datasets = sample_dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=['instruction', 'input', 'output', 'text'],\n",
    "            load_from_cache_file=False,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "\n",
    "def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        print(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 10: 100%|██████████| 5/5 [00:00<00:00, 297.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 45943, 16, 41, 15741, 14, 7448, 10, 3685, 4], [21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4, 50118], [50118, 48134, 41241, 35, 50118, 31033, 130, 4965, 13, 4959], [2245, 4, 50118, 50118, 48134, 19121, 35, 50118, 134, 4], [43800, 10, 9320, 5626, 8, 146, 686, 7, 680, 2710], [9, 12849, 8, 8942, 4, 1437, 50118, 176, 4, 30450], [4595, 7, 489, 110, 809, 2171, 8, 670, 4, 1437], [50118, 246, 4, 2315, 615, 3581, 8, 3014, 10, 4292], [3581, 3078, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 2264, 32], [5, 130, 2270, 8089, 116, 50118, 50118, 48134, 19121, 35], [50118, 133, 130, 2270, 8089, 32, 1275, 6, 2440, 6], [8, 5718, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 47066, 21700], [5, 3184, 9, 41, 37113, 4, 50118, 50118, 48134, 19121], [35, 50118, 4688, 37113, 16, 156, 62, 9, 10, 38531], [6, 61, 6308, 24892, 1790, 8, 28979, 26573, 6, 7501], [30, 36522, 14, 1504, 11, 38786, 198, 5, 38531, 4], [20, 24892, 1790, 8, 28979, 26573, 33, 10, 1313, 1427], [6, 150, 5, 36522, 33, 10, 2430, 1427, 6, 5203], [11, 41, 1374, 7974, 37113, 4, 20, 346, 9, 349], [33100, 23483, 5, 21495, 346, 8, 5, 1907, 9, 37113], [4, 2, 45943, 16, 41, 15741, 14, 7448, 10, 3685], [4, 21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4], [50118, 50118, 48134, 41241, 35, 50118, 6179, 64, 52, 1888], [935, 6631, 116, 50118, 50118, 48134, 19121, 35, 50118, 970], [32, 10, 346, 9, 1319, 7, 1888, 935, 6631, 6], [215, 25, 9255, 7, 8741, 1007, 1715, 6, 5513, 5], [304, 9, 285, 4264, 6, 27043, 5, 6574, 9, 11422], [12174, 6, 9704, 1986, 7, 1888, 5035, 31, 2683, 1715], [6, 8, 9704, 1155, 5035, 2820, 4, 6903, 6, 2172], [64, 109, 49, 233, 7, 1888, 935, 6631, 30, 4881], [512, 304, 6, 11473, 6574, 3183, 215, 25, 5627, 6], [8, 2992, 7, 1007, 5693, 14636, 4, 2, 45943, 16], [41, 15741, 14, 7448, 10, 3685, 4, 21062, 10, 1263], [14, 16574, 25830, 5, 2069, 4, 50118, 50118, 48134, 41241], [35, 50118, 47066, 21700, 10, 86, 77, 47, 56, 7], [146, 10, 1202, 568, 4, 50118, 50118, 48134, 19121, 35], [50118, 100, 56, 7, 146, 10, 1202, 568, 77, 38], [21, 447, 25, 10, 695, 1044, 23, 10, 1663, 138], [4, 38, 21, 11, 1427, 9, 10, 695, 14, 956], [7, 28, 2121, 30, 10, 1402, 1248, 11, 645, 7], [972, 5, 3653, 17, 27, 29, 2113, 4, 635, 6], [528, 7, 7152, 6091, 6, 52, 58, 45, 441, 7], [972, 5, 4267, 8, 98, 38, 56, 7, 146, 10], [1202, 568, 4, 38, 1276, 7, 4442, 5, 4267, 6], [53, 38, 56, 7, 4140, 5, 165, 17, 27, 29], [1915, 190, 617, 8, 712, 5, 1229, 4, 2223, 24], [21, 10, 11788, 568, 6, 38, 3284, 1276, 7, 213], [789, 19, 24, 7, 1306, 14, 5, 695, 21, 2121], [15, 86, 8, 14, 5, 3653, 17, 27, 29, 2113], [58, 1145, 4, 20, 695, 21, 2140, 5116, 2121, 8], [42, 21, 450, 25, 10, 17750, 7, 127, 1673, 8]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2, 45943, 16, 41, 15741, 14, 7448, 10, 3685, 4], [21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4, 50118], [50118, 48134, 41241, 35, 50118, 31033, 130, 4965, 13, 4959], [2245, 4, 50118, 50118, 48134, 19121, 35, 50118, 134, 4], [43800, 10, 9320, 5626, 8, 146, 686, 7, 680, 2710], [9, 12849, 8, 8942, 4, 1437, 50118, 176, 4, 30450], [4595, 7, 489, 110, 809, 2171, 8, 670, 4, 1437], [50118, 246, 4, 2315, 615, 3581, 8, 3014, 10, 4292], [3581, 3078, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 2264, 32], [5, 130, 2270, 8089, 116, 50118, 50118, 48134, 19121, 35], [50118, 133, 130, 2270, 8089, 32, 1275, 6, 2440, 6], [8, 5718, 4, 2, 45943, 16, 41, 15741, 14, 7448], [10, 3685, 4, 21062, 10, 1263, 14, 16574, 25830, 5], [2069, 4, 50118, 50118, 48134, 41241, 35, 50118, 47066, 21700], [5, 3184, 9, 41, 37113, 4, 50118, 50118, 48134, 19121], [35, 50118, 4688, 37113, 16, 156, 62, 9, 10, 38531], [6, 61, 6308, 24892, 1790, 8, 28979, 26573, 6, 7501], [30, 36522, 14, 1504, 11, 38786, 198, 5, 38531, 4], [20, 24892, 1790, 8, 28979, 26573, 33, 10, 1313, 1427], [6, 150, 5, 36522, 33, 10, 2430, 1427, 6, 5203], [11, 41, 1374, 7974, 37113, 4, 20, 346, 9, 349], [33100, 23483, 5, 21495, 346, 8, 5, 1907, 9, 37113], [4, 2, 45943, 16, 41, 15741, 14, 7448, 10, 3685], [4, 21062, 10, 1263, 14, 16574, 25830, 5, 2069, 4], [50118, 50118, 48134, 41241, 35, 50118, 6179, 64, 52, 1888], [935, 6631, 116, 50118, 50118, 48134, 19121, 35, 50118, 970], [32, 10, 346, 9, 1319, 7, 1888, 935, 6631, 6], [215, 25, 9255, 7, 8741, 1007, 1715, 6, 5513, 5], [304, 9, 285, 4264, 6, 27043, 5, 6574, 9, 11422], [12174, 6, 9704, 1986, 7, 1888, 5035, 31, 2683, 1715], [6, 8, 9704, 1155, 5035, 2820, 4, 6903, 6, 2172], [64, 109, 49, 233, 7, 1888, 935, 6631, 30, 4881], [512, 304, 6, 11473, 6574, 3183, 215, 25, 5627, 6], [8, 2992, 7, 1007, 5693, 14636, 4, 2, 45943, 16], [41, 15741, 14, 7448, 10, 3685, 4, 21062, 10, 1263], [14, 16574, 25830, 5, 2069, 4, 50118, 50118, 48134, 41241], [35, 50118, 47066, 21700, 10, 86, 77, 47, 56, 7], [146, 10, 1202, 568, 4, 50118, 50118, 48134, 19121, 35], [50118, 100, 56, 7, 146, 10, 1202, 568, 77, 38], [21, 447, 25, 10, 695, 1044, 23, 10, 1663, 138], [4, 38, 21, 11, 1427, 9, 10, 695, 14, 956], [7, 28, 2121, 30, 10, 1402, 1248, 11, 645, 7], [972, 5, 3653, 17, 27, 29, 2113, 4, 635, 6], [528, 7, 7152, 6091, 6, 52, 58, 45, 441, 7], [972, 5, 4267, 8, 98, 38, 56, 7, 146, 10], [1202, 568, 4, 38, 1276, 7, 4442, 5, 4267, 6], [53, 38, 56, 7, 4140, 5, 165, 17, 27, 29], [1915, 190, 617, 8, 712, 5, 1229, 4, 2223, 24], [21, 10, 11788, 568, 6, 38, 3284, 1276, 7, 213], [789, 19, 24, 7, 1306, 14, 5, 695, 21, 2121], [15, 86, 8, 14, 5, 3653, 17, 27, 29, 2113], [58, 1145, 4, 20, 695, 21, 2140, 5116, 2121, 8], [42, 21, 450, 25, 10, 17750, 7, 127, 1673, 8]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_lm_datasets = sample_tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            load_from_cache_file=False,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 55\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Training Loop\n",
    "\n",
    "[Sample Code](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py#L52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        sample_lm_datasets, shuffle=True, collate_fn=default_data_collator, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "gradient_accumulation_steps = 2\n",
    "num_warmup_steps = 5\n",
    "num_processes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "overrode_max_train_steps = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps * num_processes,\n",
    "        num_training_steps=max_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, None, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing_steps = 100\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "total_batch_size = per_device_train_batch_size * num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56 [00:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "with_tracking = True\n",
    "\n",
    "resume_from_checkpoint = False\n",
    "starting_epoch=0\n",
    "completed_steps = 0\n",
    "\n",
    "resume_step = None\n",
    "\n",
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "per_device_eval_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      " 98%|█████████▊| 55/56 [00:18<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:33<00:00, 10.96it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "        model.train()\n",
    "        if with_tracking:\n",
    "            total_loss = 0\n",
    "        if resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "        else:\n",
    "            active_dataloader = train_dataloader\n",
    "        for step, batch in enumerate(active_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # We keep track of the loss at each epoch\n",
    "                if with_tracking:\n",
    "                    total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    accelerator.save_state(output_dir)\n",
    "            if completed_steps >= max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        if eval_dataloader is not None:\n",
    "            for step, batch in enumerate(eval_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**batch)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                losses.append(accelerator.gather_for_metrics(loss.repeat(per_device_eval_batch_size)))\n",
    "\n",
    "            losses = torch.cat(losses)\n",
    "            try:\n",
    "                eval_loss = torch.mean(losses)\n",
    "                perplexity = math.exp(eval_loss)\n",
    "            except OverflowError:\n",
    "                perplexity = float(\"inf\")\n",
    "\n",
    "            print(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "            if with_tracking:\n",
    "                accelerator.log(\n",
    "                    {\n",
    "                        \"perplexity\": perplexity,\n",
    "                        \"eval_loss\": eval_loss,\n",
    "                        \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": completed_steps,\n",
    "                    },\n",
    "                    step=completed_steps,\n",
    "                )\n",
    "\n",
    "\n",
    "if with_tracking:\n",
    "    accelerator.end_training()\n",
    "\n",
    "# if args.output_dir is not None:\n",
    "#     accelerator.wait_for_everyone()\n",
    "#     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     unwrapped_model.save_pretrained(\n",
    "#         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "#     )\n",
    "#     if accelerator.is_main_process:\n",
    "#         tokenizer.save_pretrained(args.output_dir)\n",
    "#         if args.push_to_hub:\n",
    "#             api.upload_folder(\n",
    "#                 commit_message=\"End of training\",\n",
    "#                 folder_path=args.output_dir,\n",
    "#                 repo_id=repo_id,\n",
    "#                 repo_type=\"model\",\n",
    "#                 token=args.hub_token,\n",
    "#             )\n",
    "#         with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "#             json.dump({\"perplexity\": perplexity}, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\", cache_dir=\"/vol/bitbucket/rm1623/.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/vol/bitbucket/rm1623/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_params,\n",
    "        train_dataset=sample_lm_datasets,\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "        data_collator=default_data_collator,\n",
    "        # compute_metrics=compute_metrics if training_args.do_eval and not is_torch_xla_available() else None,\n",
    "        # preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "        # if training_args.do_eval and not is_torch_xla_available()\n",
    "        # else None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "***** Running training *****\n",
      "  Num examples = 55\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 55\n",
      "  Number of trainable parameters = 331,204,608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 01:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/tmp-checkpoint-25\n",
      "Configuration saved in ./results/tmp-checkpoint-25/config.json\n",
      "Configuration saved in ./results/tmp-checkpoint-25/generation_config.json\n",
      "Model weights saved in ./results/tmp-checkpoint-25/model.safetensors\n",
      "tokenizer config file saved in ./results/tmp-checkpoint-25/tokenizer_config.json\n",
      "Special tokens file saved in ./results/tmp-checkpoint-25/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/tmp-checkpoint-50\n",
      "Configuration saved in ./results/tmp-checkpoint-50/config.json\n",
      "Configuration saved in ./results/tmp-checkpoint-50/generation_config.json\n",
      "Model weights saved in ./results/tmp-checkpoint-50/model.safetensors\n",
      "tokenizer config file saved in ./results/tmp-checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./results/tmp-checkpoint-50/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=55, training_loss=0.2949854915792292, metrics={'train_runtime': 75.7708, 'train_samples_per_second': 0.726, 'train_steps_per_second': 0.726, 'total_flos': 1001108275200.0, 'train_loss': 0.2949854915792292, 'epoch': 1.0})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
